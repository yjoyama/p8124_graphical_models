---
title: "Assignment 4"
author: "Yuki Joyama (yj2803)"
output: 
  pdf_document:
    latex_engine: xelatex
header-includes:
  - \usepackage{amsmath}  # Include amsmath package
  - \newcommand{\indep}{\perp\!\!\!\perp}
  - \usepackage{setspace}
  - \setstretch{1.2}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)

library(tidyverse)
library(ggplot2)
library(pcalg)
library(huge)
```

# Problem 1
$X_i=\sum_{X_j\in Pa(X_i,\mathcal{G})}\beta_jX_j+e_i$  
Given figure 1 where all variables are jointly Gaussian, we can describe $X_1$ and $X_2$ using the linear function:    
$X_1=\beta L_1+e_1$    
$X_2=\alpha X_1 + \gamma L_1 + e_2=(\alpha\beta+\gamma)L_1+\alpha e_1+e_2$  
Where  
- $L_1$ is the common cause with variance $\sigma^2_{L_1}$  
- $e_1$ and $e_2$ are independent error term with variances $\sigma^2_1$ and $\sigma^2_2$, respectively  
- $L_1, e_1,e_2$ are independent

Now, $Cov(X_1, X_2)=Cov(X_1, (\alpha\beta+\gamma)L_1+\alpha e_1+e_2)$  
$=Cov(X_1, (\alpha\beta+\gamma)L_1)+Cov(X_1, \alpha e_1)+ Cov(X_1, e_2)$  
$=Cov(\beta L_1+e_1, (\alpha\beta+\gamma)L_1)=\beta(\alpha\beta+\gamma)Var(L_1)$  
$=\beta(\alpha\beta+\gamma)\sigma^2_{L_1}$  

For fixed $\alpha$, this can take any real value by some setting of the parameters $\beta$ and $\gamma$.   
Similarly, by $Corr(X_1,X_2)=\frac{Cov(X_1, X_2)}{\sqrt{Var(X_1)\times Var(X_2)}}$ ($Var(X_1)>0\text{ and } Var(X_2)>0$), the correlation between $X_1$ and $X_2$ can also take any value by specifying $\beta$ and $\gamma$.  

$L_1$ plays an important role in driving the observed relationship between $X_1$ and $X_2$ by introducing a backdoor pathway that can explain part or all of their covariance. If $L_1$ is omitted, the observed covariance might be misinterpreted as evidence of a direct effect ($\alpha$), when in reality, it arises from $L_1$. Moreover, the ability of $\beta$ and $\gamma$ to generate any covariance value highlights how $L_1$ can mask or amplify the true causal relationship between $X_1$ and $X_2$, demonstrating the significant power of $L_1$ in shaping their observed association.


# Problem 2
```{r}
# import file 
data <- read.table("./data.txt", header = TRUE, sep = " ", stringsAsFactors = FALSE)
```

## PC algorithm 
```{r}
# Function to calculate log likelihood
compute_log_likelihood <- function(data, adj_matrix) {
  sample_cov <- cov(data)  # Sample covariance matrix
  adj_cov <- sample_cov * adj_matrix  # Adjust covariance for graph
  
  # Regularization for numerical stability
  p <- ncol(data)
  regularization <- diag(10, p)
  adj_cov <- adj_cov + regularization
  
  # Log-likelihood calculation
  tryCatch({
    log_det <- log(det(adj_cov))  # Log determinant
    inv_cov <- solve(adj_cov)    # Inverse covariance
    log_likelihood <- -0.5 * nrow(data) * (log_det + sum(diag(inv_cov %*% sample_cov)))
    return(log_likelihood)
  }, error = function(e) {
    return(NA)  # Return NA if numerical issues occur
  })
}

# Function to calculate BIC
compute_bic <- function(pc_fit, data) {
  adj_matrix <- as(pc_fit@graph, "matrix")  # Adjacency matrix
  num_params <- sum(adj_matrix)  # Number of edges (parameters)
  log_lik <- compute_log_likelihood(data, adj_matrix)
  
  if (is.na(log_lik)) return(Inf)  # Return infinite BIC for invalid graphs
  
  n <- nrow(data)  # Sample size
  p <- ncol(data)  # Number of variables
  bic <- -2 * log_lik + log(n) * (num_params + p)  # BIC formula
  return(bic)
}

# Generate a sequence of alpha values
alphas <- seq(0.001, 0.1, by = 0.001)

# Fit PC algorithm and compute BIC for each alpha
bic_values <- sapply(alphas, function(alpha) {
  pc_fit <- pc(
    suffStat = list(C = cor(data), n = nrow(data)),
    indepTest = gaussCItest,
    alpha = alpha,
    labels = colnames(data),
    verbose = FALSE
  )
  compute_bic(pc_fit, data)
})

# Find optimal alpha
optimal_alpha <- alphas[which.min(bic_values)]
cat("Optimal alpha based on BIC:", optimal_alpha, "\n")

# Plot BIC vs Alpha
plot(
  alphas, bic_values, type = "b", pch = 16, col = "blue",
  xlab = "Alpha", ylab = "BIC",
  main = "BIC vs Alpha"
)
```

```{r, results=F}
# Run pc algorithm with alpha = 0.001
pc.fit.001 <- pc(suffStat = list(C = cor(data), n = nrow(data)),
             indepTest = gaussCItest, 
             alpha=0.001, labels = colnames(data), verbose = TRUE)

# alpha = 0.008
pc.fit.008 <- pc(suffStat = list(C = cor(data), n = nrow(data)),
             indepTest = gaussCItest, 
             alpha=0.008, labels = colnames(data), verbose = TRUE)

# alpha = 0.1
pc.fit.1 <- pc(suffStat = list(C = cor(data), n = nrow(data)),
             indepTest = gaussCItest, 
             alpha=0.1, labels = colnames(data), verbose = TRUE)
```

```{r}
# Plot CPDAG
plot(pc.fit.001@graph)
plot(pc.fit.008@graph)
plot(pc.fit.1@graph)

# Check the number of edges
pc.fit.001
pc.fit.008
pc.fit.1
```

I used the PC algorithm to estimate CPDAGs for a range of $\alpha$ values (0.001 to 0.1) to identify the optimal $\alpha$ that minimizes the Bayesian Information Criterion (BIC). For each $\alpha$, the graph structure $G(\alpha)$ was used to compute the log-likelihood $-2\ell(\hat{\Sigma}_{G'}, \hat{\mu})$, where $\ell(\cdot)$ is the log-likelihood of a Gaussian model based on the graph-constrained covariance matrix. The BIC for each graph was computed as $-2\ell + \log(n)(k + p)$, with $k$ representing the number of edges and $p$ the number of variables.  
The optimal $\alpha=0.008$ was selected as the value minimizing BIC. This model is considered to have an optimal balance between complexity and fit.

Estimated CPDAGs were plotted for specific $\alpha$ values (0.001, 0.008, 0.1), and the number of edges was examined for each plot. We see that the smaller $\alpha$ value led to sparser graph with fewer edges, and the larger $\alpha$ value led to denser graphs with more edges. 

## Graphical lasso method
I will conduct model selection using EBIC, RIC and StARS.
```{r}
# Apply graphical lasso method
out.glasso <- huge(as.matrix(data), method = "glasso")

# Perform model selection using EBIC
out.select <- huge.select(out.glasso, criterion = "ebic")

# Plot the selected graph
plot(out.glasso)

# Extract adjacency matrix of the selected graph
adj_matrix <- out.select$refit

# Check selected lambda
out.select$opt.lambda

# Visualize the selected graph
huge.plot(adj_matrix)

# Do the same using RIC
out.select <- huge.select(out.glasso, criterion = "ric")
plot(out.glasso)
adj_matrix <- out.select$refit
out.select$opt.lambda
huge.plot(adj_matrix)

# Do the same using STARS
out.select <- huge.select(out.glasso, criterion = "stars")
plot(out.glasso)
adj_matrix <- out.select$refit
out.select$opt.lambda
huge.plot(adj_matrix)
```

First, I estimated a sequence of graphs corresponding to different regularization parameters ($\lambda$). Larger $\lambda$ values produced sparser graphs with many isolated edges, while smaller $\lambda$ values led to highly connected graphs. Subsequently, three model selection criteria were applied to identify the optimal graph that balances fit and sparsity. EBIC balances log-likelihood and penalizes model complexity in high-dimensional data ($\lambda = 0.0884$). RIC uses a simpler penalty that generally favors fewer edges, resulting in a moderate sparsity ($\lambda = 0.4997$). StARS prioritizes the stability of edges across subsampled data ($\lambda = 0.1904$). We need to choose the criteria based on the nature of data and underlying relationships in the data. 






